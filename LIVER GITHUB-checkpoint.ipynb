{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDERSAMPLING METHODS\n",
    "\n",
    "https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/<br>\n",
    "    \n",
    "\n",
    "https://towardsdatascience.com/sampling-techniques-for-extremely-imbalanced-data-part-i-under-sampling-a8dbc3d8d6d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>Male</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>Male</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0   65  Female              0.7               0.1                   187   \n",
       "1   62    Male             10.9               5.5                   699   \n",
       "2   62    Male              7.3               4.1                   490   \n",
       "3   58    Male              1.0               0.4                   182   \n",
       "4   72    Male              3.9               2.0                   195   \n",
       "\n",
       "   Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                        16                          18             6.8   \n",
       "1                        64                         100             7.5   \n",
       "2                        60                          68             7.0   \n",
       "3                        14                          20             6.8   \n",
       "4                        27                          59             7.3   \n",
       "\n",
       "   Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0      3.3                        0.90        1  \n",
       "1      3.2                        0.74        1  \n",
       "2      3.3                        0.89        1  \n",
       "3      3.4                        1.00        1  \n",
       "4      2.4                        0.40        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"indian_liver_patient.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dataset'] = df['Dataset'].map({2:0,1:1}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    416\n",
       "0    167\n",
       "Name: Dataset, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>44.746141</td>\n",
       "      <td>3.298799</td>\n",
       "      <td>1.486106</td>\n",
       "      <td>290.576329</td>\n",
       "      <td>80.713551</td>\n",
       "      <td>109.910806</td>\n",
       "      <td>6.483190</td>\n",
       "      <td>3.141852</td>\n",
       "      <td>0.947064</td>\n",
       "      <td>0.713551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.189833</td>\n",
       "      <td>6.209522</td>\n",
       "      <td>2.808498</td>\n",
       "      <td>242.937989</td>\n",
       "      <td>182.620356</td>\n",
       "      <td>288.918529</td>\n",
       "      <td>1.085451</td>\n",
       "      <td>0.795519</td>\n",
       "      <td>0.319592</td>\n",
       "      <td>0.452490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>175.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>2110.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>4929.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "count  583.000000       583.000000        583.000000            583.000000   \n",
       "mean    44.746141         3.298799          1.486106            290.576329   \n",
       "std     16.189833         6.209522          2.808498            242.937989   \n",
       "min      4.000000         0.400000          0.100000             63.000000   \n",
       "25%     33.000000         0.800000          0.200000            175.500000   \n",
       "50%     45.000000         1.000000          0.300000            208.000000   \n",
       "75%     58.000000         2.600000          1.300000            298.000000   \n",
       "max     90.000000        75.000000         19.700000           2110.000000   \n",
       "\n",
       "       Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "count                583.000000                  583.000000      583.000000   \n",
       "mean                  80.713551                  109.910806        6.483190   \n",
       "std                  182.620356                  288.918529        1.085451   \n",
       "min                   10.000000                   10.000000        2.700000   \n",
       "25%                   23.000000                   25.000000        5.800000   \n",
       "50%                   35.000000                   42.000000        6.600000   \n",
       "75%                   60.500000                   87.000000        7.200000   \n",
       "max                 2000.000000                 4929.000000        9.600000   \n",
       "\n",
       "          Albumin  Albumin_and_Globulin_Ratio     Dataset  \n",
       "count  583.000000                  579.000000  583.000000  \n",
       "mean     3.141852                    0.947064    0.713551  \n",
       "std      0.795519                    0.319592    0.452490  \n",
       "min      0.900000                    0.300000    0.000000  \n",
       "25%      2.600000                    0.700000    0.000000  \n",
       "50%      3.100000                    0.930000    1.000000  \n",
       "75%      3.800000                    1.100000    1.000000  \n",
       "max      5.500000                    2.800000    1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Gender'],axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 583 entries, 0 to 582\n",
      "Data columns (total 10 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Age                         583 non-null    int64  \n",
      " 1   Total_Bilirubin             583 non-null    float64\n",
      " 2   Direct_Bilirubin            583 non-null    float64\n",
      " 3   Alkaline_Phosphotase        583 non-null    int64  \n",
      " 4   Alamine_Aminotransferase    583 non-null    int64  \n",
      " 5   Aspartate_Aminotransferase  583 non-null    int64  \n",
      " 6   Total_Protiens              583 non-null    float64\n",
      " 7   Albumin                     583 non-null    float64\n",
      " 8   Albumin_and_Globulin_Ratio  579 non-null    float64\n",
      " 9   Dataset                     583 non-null    int64  \n",
      "dtypes: float64(5), int64(5)\n",
      "memory usage: 45.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>579.000000</td>\n",
       "      <td>583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>44.746141</td>\n",
       "      <td>3.298799</td>\n",
       "      <td>1.486106</td>\n",
       "      <td>290.576329</td>\n",
       "      <td>80.713551</td>\n",
       "      <td>109.910806</td>\n",
       "      <td>6.483190</td>\n",
       "      <td>3.141852</td>\n",
       "      <td>0.947064</td>\n",
       "      <td>0.713551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.189833</td>\n",
       "      <td>6.209522</td>\n",
       "      <td>2.808498</td>\n",
       "      <td>242.937989</td>\n",
       "      <td>182.620356</td>\n",
       "      <td>288.918529</td>\n",
       "      <td>1.085451</td>\n",
       "      <td>0.795519</td>\n",
       "      <td>0.319592</td>\n",
       "      <td>0.452490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>175.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>2110.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>4929.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "count  583.000000       583.000000        583.000000            583.000000   \n",
       "mean    44.746141         3.298799          1.486106            290.576329   \n",
       "std     16.189833         6.209522          2.808498            242.937989   \n",
       "min      4.000000         0.400000          0.100000             63.000000   \n",
       "25%     33.000000         0.800000          0.200000            175.500000   \n",
       "50%     45.000000         1.000000          0.300000            208.000000   \n",
       "75%     58.000000         2.600000          1.300000            298.000000   \n",
       "max     90.000000        75.000000         19.700000           2110.000000   \n",
       "\n",
       "       Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "count                583.000000                  583.000000      583.000000   \n",
       "mean                  80.713551                  109.910806        6.483190   \n",
       "std                  182.620356                  288.918529        1.085451   \n",
       "min                   10.000000                   10.000000        2.700000   \n",
       "25%                   23.000000                   25.000000        5.800000   \n",
       "50%                   35.000000                   42.000000        6.600000   \n",
       "75%                   60.500000                   87.000000        7.200000   \n",
       "max                 2000.000000                 4929.000000        9.600000   \n",
       "\n",
       "          Albumin  Albumin_and_Globulin_Ratio     Dataset  \n",
       "count  583.000000                  579.000000  583.000000  \n",
       "mean     3.141852                    0.947064    0.713551  \n",
       "std      0.795519                    0.319592    0.452490  \n",
       "min      0.900000                    0.300000    0.000000  \n",
       "25%      2.600000                    0.700000    0.000000  \n",
       "50%      3.100000                    0.930000    1.000000  \n",
       "75%      3.800000                    1.100000    1.000000  \n",
       "max      5.500000                    2.800000    1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>40</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>98</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>52</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>245</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>184</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>216</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0     65              0.7               0.1                   187   \n",
       "1     62             10.9               5.5                   699   \n",
       "2     62              7.3               4.1                   490   \n",
       "3     58              1.0               0.4                   182   \n",
       "4     72              3.9               2.0                   195   \n",
       "..   ...              ...               ...                   ...   \n",
       "578   60              0.5               0.1                   500   \n",
       "579   40              0.6               0.1                    98   \n",
       "580   52              0.8               0.2                   245   \n",
       "581   31              1.3               0.5                   184   \n",
       "582   38              1.0               0.3                   216   \n",
       "\n",
       "     Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                          16                          18             6.8   \n",
       "1                          64                         100             7.5   \n",
       "2                          60                          68             7.0   \n",
       "3                          14                          20             6.8   \n",
       "4                          27                          59             7.3   \n",
       "..                        ...                         ...             ...   \n",
       "578                        20                          34             5.9   \n",
       "579                        35                          31             6.0   \n",
       "580                        48                          49             6.4   \n",
       "581                        29                          32             6.8   \n",
       "582                        21                          24             7.3   \n",
       "\n",
       "     Albumin  Albumin_and_Globulin_Ratio  Dataset  \n",
       "0        3.3                        0.90        1  \n",
       "1        3.2                        0.74        1  \n",
       "2        3.3                        0.89        1  \n",
       "3        3.4                        1.00        1  \n",
       "4        2.4                        0.40        1  \n",
       "..       ...                         ...      ...  \n",
       "578      1.6                        0.37        0  \n",
       "579      3.2                        1.10        1  \n",
       "580      3.2                        1.00        1  \n",
       "581      3.4                        1.00        1  \n",
       "582      4.4                        1.50        0  \n",
       "\n",
       "[583 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I filled in the missing values with median values of the column : Albumin_and_Globulin_Ratio\n",
    "\n",
    "df['Albumin_and_Globulin_Ratio']=df['Albumin_and_Globulin_Ratio'].fillna(df['Albumin_and_Globulin_Ratio'].median())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                           0\n",
       "Total_Bilirubin               0\n",
       "Direct_Bilirubin              0\n",
       "Alkaline_Phosphotase          0\n",
       "Alamine_Aminotransferase      0\n",
       "Aspartate_Aminotransferase    0\n",
       "Total_Protiens                0\n",
       "Albumin                       0\n",
       "Albumin_and_Globulin_Ratio    0\n",
       "Dataset                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum() # no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('Dataset',axis=1)\n",
    "y=df['Dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    416\n",
       "0    167\n",
       "Name: Dataset, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    416\n",
      "0    167\n",
      "Name: Dataset, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAShElEQVR4nO3df7DldX3f8efL5ZfUH4Bc7Lq7zdK4NsG2rvaWMGHSEkgTIGmWZMSBNrK1zKydwVaitWKmU01aZswkSmJ+0FkLsjgpSv0Rtg5pSxFLbSN4MSsCG+tWqdzslr0GRNBKu8u7f5zPfjnsnrt71P2ec9nzfMycOd/v+/v5Ht53BnjN9/P9lapCkiSAF0y7AUnSymEoSJI6hoIkqWMoSJI6hoIkqXPctBv4QZx++um1fv36abchSc8r99133zeqam7Utud1KKxfv56FhYVptyFJzytJ/tdy25w+kiR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkCR1ntd3NEvHsq//2l+bdgtagf7Sv/hSr7/vkYIkqWMoSJI6hoIkqWMoSJI6vYdCklVJ/iTJp9r6mUnuSfKVJB9NckKrn9jWd7Xt6/vuTZL0XJM4UngrsHNo/deB66pqA/A4cGWrXwk8XlWvBK5r4yRJE9RrKCRZC/ws8G/aeoDzgY+1IduAS9ryprZO235BGy9JmpC+jxR+C/hnwDNt/WXAN6tqX1tfBNa05TXAIwBt+xNt/HMk2ZJkIcnC0tJSn71L0szpLRSS/Bywt6ruGy6PGFpjbHu2ULW1quaran5ubuQrRiVJ36c+72g+F/j5JBcDJwEvYXDkcEqS49rRwFpgdxu/CKwDFpMcB7wUeKzH/iRJB+ntSKGq3lVVa6tqPXAZ8Omq+vvAXcDr27DNwG1teXtbp23/dFUdcqQgSerPNO5TeCfwtiS7GJwzuKHVbwBe1upvA66ZQm+SNNMm8kC8qvoM8Jm2/FXg7BFjvgtcOol+JEmjeUezJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKnT5zuaT0pyb5IvJnkwya+2+k1JvpZkR/tsbPUk+UCSXUnuT/K6vnqTJI3W50t2ngbOr6qnkhwPfDbJH7Vt76iqjx00/iJgQ/v8GHB9+5YkTUif72iuqnqqrR7fPod75/Im4Oa23+eAU5Ks7qs/SdKhej2nkGRVkh3AXuCOqrqnbbq2TRFdl+TEVlsDPDK0+2KrSZImpNdQqKr9VbURWAucneSvAu8CfgT4m8BpwDvb8Iz6iYMLSbYkWUiysLS01FPnkjSbJnL1UVV9E/gMcGFV7WlTRE8DHwLObsMWgXVDu60Fdo/4ra1VNV9V83Nzcz13Lkmzpc+rj+aSnNKWXwj8FPCnB84TJAlwCfBA22U7cEW7Cukc4Imq2tNXf5KkQ/V59dFqYFuSVQzC59aq+lSSTyeZYzBdtAP4R2387cDFwC7gO8CbeuxNkjRCb6FQVfcDrx1RP3+Z8QVc1Vc/kqQj845mSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdQwFSVLHUJAkdfp8HedJSe5N8sUkDyb51VY/M8k9Sb6S5KNJTmj1E9v6rrZ9fV+9SZJG6/NI4Wng/Kp6DbARuLC9e/nXgeuqagPwOHBlG38l8HhVvRK4ro2TJE1Qb6FQA0+11ePbp4DzgY+1+jbgkra8qa3Ttl+QJH31J0k6VK/nFJKsSrID2AvcAfxP4JtVta8NWQTWtOU1wCMAbfsTwMtG/OaWJAtJFpaWlvpsX5JmTq+hUFX7q2ojsBY4G/jRUcPa96ijgjqkULW1quaran5ubu7oNStJmszVR1X1TeAzwDnAKUmOa5vWArvb8iKwDqBtfynw2CT6kyQN9Hn10VySU9ryC4GfAnYCdwGvb8M2A7e15e1tnbb901V1yJGCJKk/xx15yPdtNbAtySoG4XNrVX0qyUPAR5L8K+BPgBva+BuADyfZxeAI4bIee5MkjdBbKFTV/cBrR9S/yuD8wsH17wKX9tWPJOnIvKNZktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktQxFCRJnT5fx7kuyV1JdiZ5MMlbW/09Sf4syY72uXhon3cl2ZXky0l+pq/eJEmj9fk6zn3A26vqC0leDNyX5I627bqq+s3hwUnOYvAKzlcDrwD+c5JXVdX+HnuUJA3p7UihqvZU1Rfa8pPATmDNYXbZBHykqp6uqq8Buxjx2k5JUn8mck4hyXoG72u+p5XekuT+JDcmObXV1gCPDO22yIgQSbIlyUKShaWlpR67lqTZ03soJHkR8HHg6qr6FnA98MPARmAP8L4DQ0fsXocUqrZW1XxVzc/NzfXUtSTNpl5DIcnxDALhD6rqEwBV9WhV7a+qZ4AP8uwU0SKwbmj3tcDuPvuTJD1Xn1cfBbgB2FlV7x+qrx4a9gvAA215O3BZkhOTnAlsAO7tqz9J0qH6vProXOCNwJeS7Gi1XwEuT7KRwdTQw8CbAarqwSS3Ag8xuHLpKq88kqTJGisUktxZVRccqTasqj7L6PMEtx9mn2uBa8fpSZJ09B02FJKcBJwMnN6uEjrwP/mXMLiXQJJ0DDnSkcKbgasZBMB9PBsK3wJ+r8e+JElTcNhQqKrfBn47yT+uqt+ZUE+SpCkZ65xCVf1Okh8H1g/vU1U399SXJGkKxj3R/GEGN5ztAA5cEVSAoSBJx5BxL0mdB86qqkPuMJYkHTvGvXntAeAv9tmIJGn6xj1SOB14KMm9wNMHilX18710JUmainFD4T19NiFJWhnGvfrov/TdiCRp+sa9+uhJnn2M9QnA8cC3q+olfTUmSZq8cY8UXjy8nuQSfCuaJB1zvq9HZ1fVHwLnH+VeJElTNu700S8Orb6AwX0L3rMgSceYca8++rtDy/sYvAdh01HvRpI0VeOeU3hT341IkqZvrHMKSdYm+WSSvUkeTfLxJGuPsM+6JHcl2ZnkwSRvbfXTktyR5Cvt+9RWT5IPJNmV5P4kr/vB/zxJ0vdi3BPNH2LwDuVXAGuAf99qh7MPeHtV/ShwDnBVkrOAa4A7q2oDcGdbB7iIwXuZNwBbgOu/h79DknQUjBsKc1X1oara1z43AXOH26Gq9lTVF9ryk8BOBoGyCdjWhm0DLmnLm4Cba+BzwClJVn9vf44k6Qcxbih8I8kvJVnVPr8E/Pm4/5Ak64HXAvcAL6+qPTAIDuCMNmwN8MjQboutdvBvbUmykGRhaWlp3BYkSWMYNxT+IfAG4H8De4DXA2OdfE7yIuDjwNVV9a3DDR1RO+Sy16raWlXzVTU/N3fYgxVJ0vdo3FD4l8DmqpqrqjMYhMR7jrRTkuMZBMIfVNUnWvnRA9NC7Xtvqy8C64Z2XwvsHrM/SdJRMG4o/PWqevzASlU9xmA6aFlJAtwA7Kyq9w9t2g5sbsubgduG6le0q5DOAZ44MM0kSZqMcW9ee0GSUw8EQ5LTxtj3XOCNwJeS7Gi1XwHeC9ya5Erg68ClbdvtwMXALuA7jDk9JUk6esYNhfcB/z3JxxjM878BuPZwO1TVZxl9ngDgghHjC7hqzH4kST0Y947mm5MsMHgIXoBfrKqHeu1MkjRx4x4p0ELAIJCkY9j39ehsSdKxyVCQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSx1CQJHUMBUlSZ+w7mo9Vf+MdN0+7Ba1A9/3GFdNuQZoKjxQkSR1DQZLUMRQkSR1DQZLU6S0UktyYZG+SB4Zq70nyZ0l2tM/FQ9velWRXki8n+Zm++pIkLa/PI4WbgAtH1K+rqo3tcztAkrOAy4BXt31+P8mqHnuTJI3QWyhU1d3AY2MO3wR8pKqerqqvMXhP89l99SZJGm0a5xTekuT+Nr10aqutAR4ZGrPYaodIsiXJQpKFpaWlvnuVpJky6VC4HvhhYCOwB3hfq2fE2Br1A1W1tarmq2p+bm6uny4laUZNNBSq6tGq2l9VzwAf5NkpokVg3dDQtcDuSfYmSZpwKCRZPbT6C8CBK5O2A5clOTHJmcAG4N5J9iZJ6vHZR0luAc4DTk+yCLwbOC/JRgZTQw8DbwaoqgeT3Ao8BOwDrqqq/X31JkkarbdQqKrLR5RvOMz4a4Fr++pHknRk3tEsSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkjqEgSeoYCpKkTm+hkOTGJHuTPDBUOy3JHUm+0r5PbfUk+UCSXUnuT/K6vvqSJC2vzyOFm4ALD6pdA9xZVRuAO9s6wEUM3su8AdgCXN9jX5KkZfQWClV1N/DYQeVNwLa2vA24ZKh+cw18Djglyeq+epMkjTbpcwovr6o9AO37jFZfAzwyNG6x1Q6RZEuShSQLS0tLvTYrSbNmpZxozohajRpYVVurar6q5ufm5npuS5Jmy6RD4dED00Lte2+rLwLrhsatBXZPuDdJmnmTDoXtwOa2vBm4bah+RbsK6RzgiQPTTJKkyTmurx9OcgtwHnB6kkXg3cB7gVuTXAl8Hbi0Db8duBjYBXwHeFNffUmSltdbKFTV5ctsumDE2AKu6qsXSdJ4VsqJZknSCmAoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6hoIkqWMoSJI6vb1k53CSPAw8CewH9lXVfJLTgI8C64GHgTdU1ePT6E+SZtU0jxR+sqo2VtV8W78GuLOqNgB3tnVJ0gStpOmjTcC2trwNuGSKvUjSTJpWKBTwn5Lcl2RLq728qvYAtO8zRu2YZEuShSQLS0tLE2pXkmbDVM4pAOdW1e4kZwB3JPnTcXesqq3AVoD5+fnqq0FJmkVTOVKoqt3tey/wSeBs4NEkqwHa995p9CZJs2zioZDkLyR58YFl4KeBB4DtwOY2bDNw26R7k6RZN43po5cDn0xy4J//b6vqPyT5PHBrkiuBrwOXTqE3SZppEw+Fqvoq8JoR9T8HLph0P5KkZ62kS1IlSVNmKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOoaCJKljKEiSOisuFJJcmOTLSXYluWba/UjSLFlRoZBkFfB7wEXAWcDlSc6ableSNDtWVCgAZwO7quqrVfV/gY8Am6bckyTNjIm/o/kI1gCPDK0vAj82PCDJFmBLW30qyZcn1NssOB34xrSbWAnym5un3YKey383D3h3jsav/NByG1ZaKIz6a+s5K1Vbga2TaWe2JFmoqvlp9yEdzH83J2elTR8tAuuG1tcCu6fUiyTNnJUWCp8HNiQ5M8kJwGXA9in3JEkzY0VNH1XVviRvAf4jsAq4saoenHJbs8RpOa1U/rs5IamqI4+SJM2ElTZ9JEmaIkNBktQxFOSjRbRiJbkxyd4kD0y7l1lhKMw4Hy2iFe4m4MJpNzFLDAX5aBGtWFV1N/DYtPuYJYaCRj1aZM2UepE0ZYaCjvhoEUmzw1CQjxaR1DEU5KNFJHUMhRlXVfuAA48W2Qnc6qNFtFIkuQX4Y+CvJFlMcuW0ezrW+ZgLSVLHIwVJUsdQkCR1DAVJUsdQkCR1DAVJUsdQkIYk2Z9kR5IHk3wxyduSHPa/kyTrk/y9Hnq5OsnJR/t3pcMxFKTn+j9VtbGqXg38HeBi4N1H2Gc9cNRDAbgaMBQ0UYaCtIyq2gtsAd6SgfVJ/muSL7TPj7eh7wV+oh1h/PJy45KsTnJ3G/dAkp9o9Z9O8sdt7L9L8qIk/wR4BXBXkrum8fdrNnnzmjQkyVNV9aKDao8DPwI8CTxTVd9NsgG4parmk5wH/NOq+rk2/uRlxr0dOKmqrm3vsTgZOBH4BHBRVX07yTuBE6vq15I8DMxX1Tcm89dLcNy0G5CeBw48SfZ44HeTbAT2A69aZvxy4z4P3JjkeOAPq2pHkr/N4OVG/y0JwAkMHusgTYWhIB1Gkr/M4H/sexmcW3gUeA2DqdfvLrPbL48aV1V3J/lbwM8CH07yG8DjwB1VdXmff4c0Ls8pSMtIMgf8a+B3azDP+lJgT1U9A7wRWNWGPgm8eGjXkeOS/BCwt6o+CNwAvA74HHBukle2MScnedUyvyv1ziMF6blemGQHgymgfcCHgfe3bb8PfDzJpcBdwLdb/X5gX5IvMnin8HLjzgPekeT/AU8BV1TVUpJ/ANyS5MQ27p8D/wPYCvxRkj1V9ZM9/b3Sc3iiWZLUcfpIktQxFCRJHUNBktQxFCRJHUNBktQxFCRJHUNBktT5/4j6C0JkNaJlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.shape,y.shape\n",
    "sns.countplot(y,label=\"count\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETECTING HIGHLY CORRELATED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>40</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>98</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>52</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>245</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>184</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>216</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0     65              0.7               0.1                   187   \n",
       "1     62             10.9               5.5                   699   \n",
       "2     62              7.3               4.1                   490   \n",
       "3     58              1.0               0.4                   182   \n",
       "4     72              3.9               2.0                   195   \n",
       "..   ...              ...               ...                   ...   \n",
       "578   60              0.5               0.1                   500   \n",
       "579   40              0.6               0.1                    98   \n",
       "580   52              0.8               0.2                   245   \n",
       "581   31              1.3               0.5                   184   \n",
       "582   38              1.0               0.3                   216   \n",
       "\n",
       "     Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                          16                          18             6.8   \n",
       "1                          64                         100             7.5   \n",
       "2                          60                          68             7.0   \n",
       "3                          14                          20             6.8   \n",
       "4                          27                          59             7.3   \n",
       "..                        ...                         ...             ...   \n",
       "578                        20                          34             5.9   \n",
       "579                        35                          31             6.0   \n",
       "580                        48                          49             6.4   \n",
       "581                        29                          32             6.8   \n",
       "582                        21                          24             7.3   \n",
       "\n",
       "     Albumin  Albumin_and_Globulin_Ratio  \n",
       "0        3.3                        0.90  \n",
       "1        3.2                        0.74  \n",
       "2        3.3                        0.89  \n",
       "3        3.4                        1.00  \n",
       "4        2.4                        0.40  \n",
       "..       ...                         ...  \n",
       "578      1.6                        0.37  \n",
       "579      3.2                        1.10  \n",
       "580      3.2                        1.00  \n",
       "581      3.4                        1.00  \n",
       "582      4.4                        1.50  \n",
       "\n",
       "[583 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "578    0\n",
       "579    1\n",
       "580    1\n",
       "581    1\n",
       "582    0\n",
       "Name: Dataset, Length: 583, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST on the Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [1, 23, 45],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [20, 65, 110, 155, 200]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10]\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split\n",
    "               \n",
    "              \n",
    "              \n",
    "              }\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting Training Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bbeaa391a0432bb5d216999f36883a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 5 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 6 folds for each of 10 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  60 | elapsed:    1.0s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 6 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 7 folds for each of 10 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  70 | elapsed:    1.3s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 7 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 8 folds for each of 10 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  80 | elapsed:    1.6s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 8 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 9 folds for each of 10 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 9 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  77 out of 100 | elapsed:    1.9s remaining:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "\n",
      "Out of Loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = cvx, verbose=2, random_state=42, n_jobs = -1, scoring='accuracy')\n",
    "   \n",
    "    rf_random.fit(x,y)\n",
    "    \n",
    "    print(\"Accuracy for %d fold CV = %3.2f\"%(cvx,rf_random.score(x,y)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (rf_random.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting Training Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855c0af066044e618526caf25fcf32a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 5 fold CV = 1.00\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 6 folds for each of 10 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 6 fold CV = 1.00\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 7 folds for each of 10 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  70 | elapsed:    1.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 7 fold CV = 1.00\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 8 folds for each of 10 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 8 fold CV = 1.00\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 9 folds for each of 10 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  67 out of  90 | elapsed:    1.7s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 9 fold CV = 1.00\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for 10 fold CV = 1.00\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      " \n",
      " \n",
      "\n",
      "Out of Loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = cvx, verbose=2, random_state=42, n_jobs = -1, scoring='recall')\n",
    "   \n",
    "    rf_random.fit(x,y)\n",
    "    \n",
    "    print(\"Recall for %d fold CV = %3.2f\"%(cvx,rf_random.score(x,y)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (rf_random.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision  Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting Training Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c111c2922824755a21bc389ff711f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 5 fold CV = 0.94\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      " \n",
      " \n",
      "Fitting 6 folds for each of 10 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  60 | elapsed:    1.0s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 6 fold CV = 0.96\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 155, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      " \n",
      " \n",
      "Fitting 7 folds for each of 10 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  70 | elapsed:    1.2s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 7 fold CV = 0.94\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      " \n",
      " \n",
      "Fitting 8 folds for each of 10 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 8 fold CV = 0.96\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 110, 'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 45}\n",
      " \n",
      " \n",
      "Fitting 9 folds for each of 10 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 9 fold CV = 0.94\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      " \n",
      " \n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 10 fold CV = 0.94\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'n_estimators': 20, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      " \n",
      " \n",
      "\n",
      "Out of Loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.7s finished\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = cvx, verbose=2, random_state=42, n_jobs = -1, scoring='precision')\n",
    "   \n",
    "    rf_random.fit(x,y)\n",
    "    \n",
    "    print(\"Precision for %d fold CV = %3.2f\"%(cvx,rf_random.score(x,y)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (rf_random.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "df_scaled = pd.DataFrame(SS.fit_transform(x), columns = x.columns) # as scaling mandotory for KNN model \n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.3, random_state = 1)\n",
    "x_train1,x_test1,y_train,y_test = train_test_split(df_scaled,y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "l= []  #List to store the various model metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((408, 9), (175, 9))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.29%\n",
      "Precision : 75.38%\n",
      "Recall: 78.40%\n",
      "F1 : 76.86%\n",
      "AUC : 57.20%\n"
     ]
    }
   ],
   "source": [
    "# XG Boost\n",
    " \n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "f1 = f1_score(y_test, predictions)\n",
    "re = recall_score(y_test, predictions)\n",
    "pre = precision_score(y_test, predictions)\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Precision : %.2f%%\" % (pre * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (re * 100.0))\n",
    "print(\"F1 : %.2f%%\" % (f1 * 100.0))\n",
    "print(\"AUC : %.2f%%\" % (auc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.00%\n",
      "Precision : 74.47%\n",
      "Recall: 84.00%\n",
      "F1 : 78.95%\n",
      "AUC : 56.00%\n"
     ]
    }
   ],
   "source": [
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200\n",
    ")\n",
    "classifier.fit(x_train, y_train)\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "re = recall_score(y_test, predictions)\n",
    "pre = precision_score(y_test, predictions)\n",
    "auc =roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Precision : %.2f%%\" % (pre * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (re * 100.0))\n",
    "print(\"F1 : %.2f%%\" % (f1 * 100.0))\n",
    "print(\"AUC : %.2f%%\" % (auc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_lr(x,y):\n",
    "    mod = {}\n",
    "    model = LogisticRegression().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'LogisticRegression'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_lr(x_train,y_train))\n",
    "\n",
    "def models_dt(x,y):\n",
    "    mod = {}\n",
    "    model = DecisionTreeClassifier().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'Decision Tree'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_dt(x_train,y_train))\n",
    "\n",
    "def models_rf(x,y):\n",
    "    mod = {}\n",
    "    model = RandomForestClassifier().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'Random Forest'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_rf(x_train,y_train))\n",
    "\n",
    "def models_nb(x,y):\n",
    "    mod = {}\n",
    "    model = GaussianNB().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'GaussianNB'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_nb(x_train,y_train))\n",
    "\n",
    "def models_knn(x,y):\n",
    "    mod = {}\n",
    "    model = KNeighborsClassifier().fit(x,y)\n",
    "    ypred = model.predict(x_test1)\n",
    "    mod['Model'] = 'KNN'\n",
    "    mod['Train_Score'] = model.score(x_train1,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_knn(x_train1,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_Score</th>\n",
       "      <th>Test_accuracy</th>\n",
       "      <th>f1score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.727941</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.826855</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697143</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.534314</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.774436</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.730496</td>\n",
       "      <td>0.532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Train_Score  Test_accuracy   f1score  recall  \\\n",
       "0  LogisticRegression     0.727941       0.720000  0.826855   0.936   \n",
       "1       Decision Tree     1.000000       0.622857  0.731707   0.720   \n",
       "2       Random Forest     1.000000       0.697143  0.800000   0.848   \n",
       "3          GaussianNB     0.534314       0.611429  0.634409   0.472   \n",
       "4                 KNN     0.784314       0.657143  0.774436   0.824   \n",
       "\n",
       "   precision  roc_auc  \n",
       "0   0.740506    0.558  \n",
       "1   0.743802    0.550  \n",
       "2   0.757143    0.584  \n",
       "3   0.967213    0.716  \n",
       "4   0.730496    0.532  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = pd.DataFrame(l)\n",
    "base_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM_LR wihtout UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr=x\n",
    "ytr=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((583, 9), (583,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr.shape, ytr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature shape:  (466, 9)\n",
      "test feature shape:  (117, 9)\n"
     ]
    }
   ],
   "source": [
    "xtr_train, xtr_test, ytr_train, ytr_test = train_test_split(xtr,ytr,test_size=0.2,random_state=20)\n",
    "print (\"train feature shape: \", xtr_train.shape)\n",
    "print (\"test feature shape: \",xtr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7188841201716738"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lOGISTICS REGRESSION ACCURACY\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xtr_train,ytr_train)\n",
    "y_pred=logitmodeldf.predict(xtr_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytr_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481481481481481"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lOGISTICS REGRESSION PRECISION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xtr_train,ytr_train)\n",
    "y_pred=logitmodeldf.predict(xtr_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(ytr_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9126506024096386"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lOGISTICS REGRESSION RECALL\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xtr_train,ytr_train)\n",
    "y_pred=logitmodeldf.predict(xtr_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(ytr_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5757282862794462"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lOGISTICS REGRESSION AUC_SCORE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xtr_train,ytr_train)\n",
    "y_pred=logitmodeldf.predict(xtr_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(ytr_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FULL DATASET in Gridsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting Training Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5b9f58349e486cb4e4e4497f645adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for 2 fold CV = 0.77\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 49, 'SupVM__gamma': 0.05}\n",
      " \n",
      " \n",
      "Score for 3 fold CV = 0.71\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 100, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 4 fold CV = 0.75\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.07}\n",
      " \n",
      " \n",
      "Score for 5 fold CV = 0.74\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 0.5, 'SupVM__gamma': 0.5}\n",
      " \n",
      " \n",
      "Score for 6 fold CV = 0.76\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.1}\n",
      " \n",
      " \n",
      "Score for 7 fold CV = 0.77\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 30, 'SupVM__gamma': 0.07}\n",
      " \n",
      " \n",
      "Score for 8 fold CV = 0.75\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.07}\n",
      " \n",
      " \n",
      "Score for 9 fold CV = 0.76\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.1}\n",
      " \n",
      " \n",
      "\n",
      "Out of Loop\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= {\n",
    "    \n",
    "    'SupVM__C': [0.1, 0.5, 1, 10,30, 40,41,42,43,46,49, 50, 75, 100, 500, 1000], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(2,10)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(xtr,ytr)\n",
    "    \n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(xtr,ytr)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for 7 fold CV = 0.77\n",
    "# Without precision\n",
    "\n",
    "svc=SVC(kernel='rbf',gamma=0.07)\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred=svc.predict(x_test)\n",
    "acu=metrics.accuracy_score(y_test,y_pred)\n",
    "print(acu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7209302325581395\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for 7 fold CV = 0.77\n",
    "# Without precision\n",
    "\n",
    "svc=SVC(kernel='rbf',gamma=0.07)\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred=svc.predict(x_test)\n",
    "py=metrics.precision_score(y_test,y_pred)\n",
    "print(py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992\n"
     ]
    }
   ],
   "source": [
    "# Without recall\n",
    "svc=SVC(kernel='rbf',gamma=0.07)\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred=svc.predict(x_test)\n",
    "auc=metrics.recall_score(y_test,y_pred)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.516\n"
     ]
    }
   ],
   "source": [
    "# Without auc\n",
    "svc=SVC(kernel='rbf',gamma=0.07)\n",
    "svc.fit(x_train,y_train)\n",
    "y_pred=svc.predict(x_test)\n",
    "auc=metrics.roc_auc_score(y_test,y_pred)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try out various undersampling technqiues to address the class imbalance and then perform the union and intersection operations on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods that Select Examples to Keep\n",
    "1.Near Miss Undersampling<br>\n",
    "2.Condensed Nearest Neighbor Rule for Undersampling <br><br>\n",
    "\n",
    "## Methods that Select Examples to Delete\n",
    "1.Tomek Links for Undersampling<br>\n",
    "2.Edited Nearest Neighbors Rule for Undersampling<br><br>\n",
    "\n",
    "## Combinations of Keep and Delete Methods\n",
    "1.One-Sided Selection for Undersampling<br>\n",
    "2.Neighborhood Cleaning Rule for Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import (RandomUnderSampler, ClusterCentroids, TomekLinks, NeighbourhoodCleaningRule, NearMiss)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Bilirubin</th>\n",
       "      <th>Direct_Bilirubin</th>\n",
       "      <th>Alkaline_Phosphotase</th>\n",
       "      <th>Alamine_Aminotransferase</th>\n",
       "      <th>Aspartate_Aminotransferase</th>\n",
       "      <th>Total_Protiens</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Albumin_and_Globulin_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>187</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>699</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>490</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>182</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195</td>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>60</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>500</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>5.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>40</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>98</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>52</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>245</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>184</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>6.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>216</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Total_Bilirubin  Direct_Bilirubin  Alkaline_Phosphotase  \\\n",
       "0     65              0.7               0.1                   187   \n",
       "1     62             10.9               5.5                   699   \n",
       "2     62              7.3               4.1                   490   \n",
       "3     58              1.0               0.4                   182   \n",
       "4     72              3.9               2.0                   195   \n",
       "..   ...              ...               ...                   ...   \n",
       "578   60              0.5               0.1                   500   \n",
       "579   40              0.6               0.1                    98   \n",
       "580   52              0.8               0.2                   245   \n",
       "581   31              1.3               0.5                   184   \n",
       "582   38              1.0               0.3                   216   \n",
       "\n",
       "     Alamine_Aminotransferase  Aspartate_Aminotransferase  Total_Protiens  \\\n",
       "0                          16                          18             6.8   \n",
       "1                          64                         100             7.5   \n",
       "2                          60                          68             7.0   \n",
       "3                          14                          20             6.8   \n",
       "4                          27                          59             7.3   \n",
       "..                        ...                         ...             ...   \n",
       "578                        20                          34             5.9   \n",
       "579                        35                          31             6.0   \n",
       "580                        48                          49             6.4   \n",
       "581                        29                          32             6.8   \n",
       "582                        21                          24             7.3   \n",
       "\n",
       "     Albumin  Albumin_and_Globulin_Ratio  \n",
       "0        3.3                        0.90  \n",
       "1        3.2                        0.74  \n",
       "2        3.3                        0.89  \n",
       "3        3.4                        1.00  \n",
       "4        2.4                        0.40  \n",
       "..       ...                         ...  \n",
       "578      1.6                        0.37  \n",
       "579      3.2                        1.10  \n",
       "580      3.2                        1.00  \n",
       "581      3.4                        1.00  \n",
       "582      4.4                        1.50  \n",
       "\n",
       "[583 rows x 9 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "578    0\n",
       "579    1\n",
       "580    1\n",
       "581    1\n",
       "582    0\n",
       "Name: Dataset, Length: 583, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDITED NEAREST NETWORKS  ( ENN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "x5=x\n",
    "y5=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enn=EditedNearestNeighbours()\n",
    "x_enn,y_enn = enn.fit_resample(x5,y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 167, 1: 217})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter=Counter(y_enn)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "enndf=pd.concat([x_enn,y_enn], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 217, 0: 167})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_enn)\n",
    "counter=Counter(y_enn)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature shape:  (307, 9)\n",
      "test feature shape:  (77, 9)\n"
     ]
    }
   ],
   "source": [
    "xe_train, xe_test, ye_train, ye_test = train_test_split(x_enn,y_enn,test_size=0.2,random_state=20)\n",
    "print (\"train feature shape: \", xe_train.shape)\n",
    "print (\"test feature shape: \",xe_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8701298701298701"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lOGISTICS REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xe_train,ye_train)\n",
    "y_pred=logitmodeldf.predict(xe_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ye_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8098958333333334"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=0,solver='liblinear').fit(x_enn, y_enn)\n",
    " \n",
    "\n",
    "classifier.score(x_enn,y_enn).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74358974 0.74358974 0.82051282 0.76923077 0.76315789 0.68421053\n",
      " 0.76315789 0.73684211 0.76315789 0.81578947]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "svc=SVC(kernel='rbf')\n",
    "scores=cross_val_score(svc,x_enn,y_enn,cv=10,scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting Training Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cc4b6a04a9425d8657076e2f04b5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for 2 fold CV = 0.82\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.01}\n",
      " \n",
      " \n",
      "Score for 3 fold CV = 0.82\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 46, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 4 fold CV = 0.83\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.05}\n",
      " \n",
      " \n",
      "Score for 5 fold CV = 0.83\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 1000, 'SupVM__gamma': 0.001}\n",
      " \n",
      " \n",
      "Score for 6 fold CV = 0.82\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 43, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 7 fold CV = 0.82\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 30, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 8 fold CV = 0.84\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 10, 'SupVM__gamma': 0.07}\n",
      " \n",
      " \n",
      "Score for 9 fold CV = 0.82\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 75, 'SupVM__gamma': 0.001}\n",
      " \n",
      " \n",
      "Score for 10 fold CV = 0.82\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 500, 'SupVM__gamma': 0.001}\n",
      " \n",
      " \n",
      "\n",
      "Out of Loop\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= {\n",
    "    \n",
    "    'SupVM__C': [0.1, 0.5, 1, 10,30, 40,41,42,43,46,49, 50, 75, 100, 500, 1000], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(2,11)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(x_enn,y_enn)\n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(x_enn,y_enn)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in d:\\anaconda\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from xgboost) (1.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.31%\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(xe_train, ye_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(xe_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(ye_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NEIGHBOURHOOD CLEANING \n",
    "\n",
    "The Neighborhood Cleaning Rule, or NCR for short, is an undersampling technique that combines both the Condensed Nearest Neighbor (CNN) Rule to remove redundant examples and the Edited Nearest Neighbors (ENN) Rule to remove noisy or ambiguous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 281, 0: 167})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import NeighbourhoodCleaningRule \n",
    "ncr = NeighbourhoodCleaningRule()\n",
    "x_ncr, y_ncr = ncr.fit_resample(x, y)\n",
    "counter=Counter(y_ncr)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncrdf=pd.concat([x_ncr,y_ncr],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x211c01abb08>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_ncr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796875"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=0,solver='liblinear').fit(x_ncr, y_ncr)\n",
    " \n",
    "\n",
    "classifier.score(x_ncr,y_ncr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature shape:  (358, 9)\n",
      "test feature shape:  (90, 9)\n"
     ]
    }
   ],
   "source": [
    "xncr_train, xncr_test, yncr_train, yncr_test = train_test_split(x_ncr,y_ncr,test_size=0.2,random_state=20)\n",
    "print (\"train feature shape: \", xncr_train.shape)\n",
    "print (\"test feature shape: \",xncr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666667"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lOGISTICS REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xncr_train,yncr_train)\n",
    "y_pred=logitmodeldf.predict(xncr_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(yncr_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Fitting Training Data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39af5bb9c5574c099804c8dd50f9f9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for 5 fold CV = 0.81\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 49, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 6 fold CV = 0.79\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 30, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 7 fold CV = 0.81\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 49, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n",
      "Score for 8 fold CV = 0.80\n",
      "!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\n",
      "{'SupVM__C': 40, 'SupVM__gamma': 0.005}\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= { \n",
    "    'SupVM__C': [0.1, 0.5, 1, 10,30, 40,41,42,43,46,49,], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(x_ncr,y_ncr)\n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(x_ncr,y_ncr)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(xncr_train, yncr_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(xncr_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(yncr_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RANDOM UNDER SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XN=x\n",
    "YN=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus=RandomUnderSampler(random_state=42)\n",
    "\n",
    "x_rus,y_rus=rus.fit_resample(XN,YN)\n",
    "from collections import Counter\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(y_rus)))\n",
    "sns.countplot(y_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rusdf = pd.concat([x_rus,y_rus],axis=1)\n",
    "rusdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=0,solver='liblinear').fit(x_rus, y_rus)\n",
    " \n",
    "\n",
    "classifier.score(x_rus,y_rus).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rus_train,x_rus_test, y_rus_train, y_rus_test = train_test_split(x_rus,y_rus,test_size=0.33,random_state=20)\n",
    "print (\"train feature shape: \", xncr_train.shape)\n",
    "print (\"test feature shape: \",xncr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(x_rus_train,y_rus_train)\n",
    "y_pred=logitmodeldf.predict(x_rus_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_rus_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= { \n",
    "    'SupVM__C': [0.001,0.1,0.2, 0.5, 1,2,3,4, 10,30, 40,41,42,43,46,49, 50, 75, 100, 500, 1000], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(x_rus,y_rus)\n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(x_rus,y_rus)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_rus_train, y_rus_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(x_rus_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_rus_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.CONDENSED NEAREST NEIGHBOUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3=x\n",
    "y3=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "cnn = CondensedNearestNeighbour(n_neighbors=1)\n",
    "\n",
    "x_cnn,y_cnn= cnn.fit_resample(x3, y3)\n",
    "cnndf=pd.concat([x_cnn,y_cnn],axis=1)\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(y_rus)))\n",
    "sns.countplot(y_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=0,solver='liblinear').fit(x_cnn, y_cnn)\n",
    " \n",
    "\n",
    "classifier.score(x_cnn,y_cnn).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cnn_train,x_cnn_test, y_cnn_train, y_cnn_test = train_test_split(x_cnn,y_cnn,test_size=0.33,random_state=20)\n",
    "print (\"train feature shape: \", x_cnn_train.shape)\n",
    "print (\"test feature shape: \",x_cnn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(x_cnn_train,y_cnn_train)\n",
    "y_pred=logitmodeldf.predict(x_cnn_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_cnn_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= { \n",
    "    'SupVM__C': [0.001,0.1,0.2, 0.5, 1,2,3,4, 10,30, 40,41,42,43,46,49, 50, 75, 100, 500, 1000], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(x_cnn,y_cnn)\n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(x_cnn,y_cnn)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near Miss 3\n",
    "\n",
    "NearMiss-2 selects examples from the majority class that have the smallest average distance to the three furthest examples from the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XV=x\n",
    "YV=y\n",
    "\n",
    "nearmisssampler3 = NearMiss(version=3)\n",
    "x_nm3, y_nm3 = nearmisssampler3.fit_sample(XV, YV)\n",
    "sns.countplot(y_nm3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=0,solver='liblinear').fit(x_nm3, y_nm3)\n",
    " \n",
    "\n",
    "classifier.score(x_nm3,y_nm3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nm3_train,x_nm3_test, y_nm3_train, y_nm3_test = train_test_split(x_nm3,y_nm3,test_size=0.33,random_state=20)\n",
    "print (\"train feature shape: \", x_nm3_train.shape)\n",
    "print (\"test feature shape: \",x_nm3_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(x_nm3_train,y_nm3_train)\n",
    "y_pred=logitmodeldf.predict(x_nm3_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_nm3_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= { \n",
    "    'SupVM__C': [0.001,0.1,0.2, 0.5, 1,2,3,4, 10,30, 40,41,42,43,46,49, 50, 75, 100, 500, 1000], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(x_nm3,y_nm3)\n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(x_nm3,y_nm3)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NM ^ ENN) U (ENN ^ NCR) U (NCR ^ NM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import (RandomUnderSampler, ClusterCentroids, TomekLinks, NeighbourhoodCleaningRule, NearMiss)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NM_3\n",
    "\n",
    "x2=x\n",
    "y2=y\n",
    "nearmisssampler3 = NearMiss(version=3)\n",
    "x_nm3, y_nm3 = nearmisssampler3.fit_sample(x2, y2)\n",
    "nm3df=pd.concat([x_nm3,y_nm3],axis=1)\n",
    "\n",
    "nm3df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENN\n",
    "\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "x5=x\n",
    "y5=y\n",
    "enn=EditedNearestNeighbours()\n",
    "x_enn,y_enn = enn.fit_resample(x5,y5)\n",
    "enndf=pd.concat([x_enn,y_enn], axis=1)\n",
    "\n",
    "\n",
    "enndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NCR\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule \n",
    "ncr = NeighbourhoodCleaningRule()\n",
    "x_ncr, y_ncr = ncr.fit_resample(x, y)\n",
    "ncrdf=pd.concat([x_ncr,y_ncr],axis=1)\n",
    "\n",
    "\n",
    "ncrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (NM ^ ENN) U (ENN ^ NCR) U (NCR ^ NM)\n",
    "\n",
    "ne=nm3df.merge(enndf)\n",
    "en=enndf.merge(ncrdf)\n",
    "nn=ncrdf.merge(nm3df)\n",
    "\n",
    "\n",
    "hola = pd.concat([ne,en,nn], ignore_index = True)\n",
    "hola = hola.drop_duplicates()\n",
    "\n",
    "xdc=hola.drop('Dataset',axis=1)\n",
    "ydc=hola['Dataset']\n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "print('Resampled dataset shape {}'.format(Counter(ydc)))\n",
    "sns.countplot(ydc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdc_train, xdc_test, ydc_train, ydc_test = train_test_split(xdc,ydc,test_size=0.2,random_state=20)\n",
    "print (\"train feature shape: \", xdc_train.shape)\n",
    "print (\"test feature shape: \",xdc_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION ACCURACY\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xdc_train,ydc_train)\n",
    "y_pred=logitmodeldf.predict(xdc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ydc_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION PRECISION \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xdc_train,ydc_train)\n",
    "y_pred=logitmodeldf.predict(xdc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(ydc_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION RECALLL\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xdc_train,ydc_train)\n",
    "y_pred=logitmodeldf.predict(xdc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(ydc_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lOGISTICS REGRESSION AUC_SCORE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV \n",
    "logitmodeldf = LogisticRegression(solver='liblinear').fit(xdc_train,ydc_train)\n",
    "y_pred=logitmodeldf.predict(xdc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(ydc_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipe_steps = [('scaler', StandardScaler()), ('SupVM', SVC(kernel='rbf'))]\n",
    "\n",
    "\n",
    "check_params= { \n",
    "    'SupVM__C': [0.001,0.005,0.01,0.09,0.1, 0.5,0.8,1,2,3,4,5, 10,30,40,41,42,43], \n",
    "    'SupVM__gamma' : [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5, 1, 5, 10, 50]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    newgrid=GridSearchCV(pipeline,param_grid=check_params,cv=cvx)\n",
    "   \n",
    "    newgrid.fit(xdc,ydc)\n",
    "    \n",
    "    print(\"Score for %d fold CV = %3.2f\"%(cvx,newgrid.score(xdc,ydc)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (newgrid.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Boost\n",
    " \n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    " \n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(xdc_train, ydc_train)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(xdc_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(ydc_test, predictions)\n",
    "\n",
    "f1 = f1_score(ydc_test, predictions)\n",
    "re = recall_score(ydc_test, predictions)\n",
    "pre = precision_score(ydc_test, predictions)\n",
    "auc =roc_auc_score(ydc_test, predictions)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Precision : %.2f%%\" % (pre * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (re * 100.0))\n",
    "print(\"F1 : %.2f%%\" % (f1 * 100.0))\n",
    "print(\"auc : %.2f%%\" % (auc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Scale the data to be between -1 and 1\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(xdc_train)\n",
    "xdc_train = scaler.transform(xdc_train)\n",
    "xdc_test = scaler.transform(xdc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200\n",
    ")\n",
    "classifier.fit(xdc_train, ydc_train)\n",
    "predictions = classifier.predict(xdc_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(ydc_test, predictions)\n",
    "f1 = f1_score(ydc_test, predictions)\n",
    "re = recall_score(ydc_test, predictions)\n",
    "pre = precision_score(ydc_test, predictions)\n",
    "auc =roc_auc_score(ydc_test, predictions)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"Precision : %.2f%%\" % (pre * 100.0))\n",
    "print(\"Recall: %.2f%%\" % (re * 100.0))\n",
    "print(\"F1 : %.2f%%\" % (f1 * 100.0))\n",
    "print(\"auc : %.2f%%\" % (auc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "df_scaled = pd.DataFrame(SS.fit_transform(xdc), columns = xdc.columns) # as scaling mandotory for KNN model \n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(xdc,ydc, test_size = 0.3, random_state = 1)\n",
    "x_train1,x_test1,y_train,y_test = train_test_split(df_scaled,ydc, test_size = 0.3, random_state = 1)\n",
    "\n",
    "l= []  #List to store the various model metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_lr(x,y):\n",
    "    mod = {}\n",
    "    model = LogisticRegression().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'LogisticRegression'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_lr(x_train,y_train))\n",
    "\n",
    "def models_dt(x,y):\n",
    "    mod = {}\n",
    "    model = DecisionTreeClassifier().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'Decision Tree'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_dt(x_train,y_train))\n",
    "\n",
    "def models_rf(x,y):\n",
    "    mod = {}\n",
    "    model = RandomForestClassifier().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'Random Forest'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_rf(x_train,y_train))\n",
    "\n",
    "def models_nb(x,y):\n",
    "    mod = {}\n",
    "    model = GaussianNB().fit(x,y)\n",
    "    ypred = model.predict(x_test)\n",
    "    mod['Model'] = 'GaussianNB'\n",
    "    mod['Train_Score'] = model.score(x_train,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_nb(x_train,y_train))\n",
    "\n",
    "def models_knn(x,y):\n",
    "    mod = {}\n",
    "    model = KNeighborsClassifier().fit(x,y)\n",
    "    ypred = model.predict(x_test1)\n",
    "    mod['Model'] = 'KNN'\n",
    "    mod['Train_Score'] = model.score(x_train1,y_train)\n",
    "    mod['Test_accuracy'] = metrics.accuracy_score(y_test,ypred)\n",
    "    mod['f1score'] = metrics.f1_score(y_test,ypred)\n",
    "    mod['recall'] = metrics.recall_score(y_test, ypred)\n",
    "    mod['precision'] = metrics.precision_score(y_test, ypred)\n",
    "    model.predict_proba(x_test)\n",
    "    mod['roc_auc'] = metrics.roc_auc_score(y_test,ypred)\n",
    "    return mod\n",
    "l.append(models_knn(x_train1,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_df = pd.DataFrame(l)\n",
    "base_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (｡◕‿◕｡)  (｡◕‿◕｡) (｡◕‿◕｡)  (｡◕‿◕｡)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (~˘▾˘)~  ಠ‿↼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST ++ RANDOM_SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = cvx, verbose=2, random_state=42, n_jobs = -1, scoring='accuracy')\n",
    "   \n",
    "    rf_random.fit(xdc,ydc)\n",
    "    \n",
    "    print(\"Accuracy for %d fold CV = %3.2f\"%(cvx,rf_random.score(xdc,ydc)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (rf_random.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = cvx, verbose=2, random_state=42, n_jobs = -1, scoring='recall')\n",
    "   \n",
    "    rf_random.fit(xdc,ydc)\n",
    "    \n",
    "    print(\"RECALL for %d fold CV = %3.2f\"%(cvx,rf_random.score(xdc,ydc)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (rf_random.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline(pipe_steps)\n",
    "# I love You So Much \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "rf=RandomForestClassifier(random_state=0)\n",
    "print(\"Start Fitting Training Data\")\n",
    "for cvx in tqdm(range(5,11)):\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = cvx, verbose=2, random_state=42, n_jobs = -1, scoring='precision')\n",
    "   \n",
    "    rf_random.fit(xdc,ydc)\n",
    "    \n",
    "    print(\"PRECISION for %d fold CV = %3.2f\"%(cvx,rf_random.score(xdc,ydc)))\n",
    "    print (\"!!!!!!!! Best-Fit Parameters From Training Data !!!!!!!!!!!!!!\")\n",
    "    print (rf_random.best_params_)\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "print(\"Out of Loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
